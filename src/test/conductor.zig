//! The Conductor coordinates a set of Clients, scheduling requests (generated by the Workload),
//! and receiving replies (validated by the Workload).
//!
//! Replies from the cluster may arrive out-of-order; the Conductor reassembles them in the
//! correct order (by ascending op number) before passing them into the Workload.
const std = @import("std");
const assert = std.debug.assert;
const log = std.log.scoped(.test_conductor);

const vsr = @import("../vsr.zig");
const util = @import("../util.zig");
const constants = @import("../constants.zig");
const IdPermutation = @import("id.zig").IdPermutation;
const MessagePool = @import("../message_pool.zig").MessagePool;
const Message = MessagePool.Message;
const Cluster = @import("cluster.zig").Cluster;
const Client = @import("cluster.zig").Client;
const StateMachine = @import("cluster.zig").StateMachine;

// TODO(zig) This won't be necessary in Zig 0.10.
const PriorityQueue = @import("./priority_queue.zig").PriorityQueue;

/// Both messages belong to the Conductor's `MessagePool`.
const PendingReply = struct {
    client_index: usize,
    request: *Message,
    reply: *Message,

    /// `PendingReply`s are ordered by ascending reply op.
    fn compare(context: void, a: PendingReply, b: PendingReply) std.math.Order {
        _ = context;
        return std.math.order(a.reply.header.op, b.reply.header.op);
    }
};

const PendingReplyQueue = PriorityQueue(PendingReply, void, PendingReply.compare);

const RequestUserData = extern struct {
    conductor: *Conductor,
    padding: [8]u8 = [_]u8{0} ** 8,
};

pub const Conductor = struct {
    /// Reply messages (from cluster to client) may be reordered during transit.
    /// The Conductor must reassemble them in the original order (ascending op/commit
    /// number) before handing them off to the Workload for verification.
    ///
    /// `Conduction.stalled_queue` hold replies (and corresponding requests) that are
    /// waiting to be processed.
    pub const stalled_queue_capacity =
        constants.clients_max * constants.client_request_queue_max * 2;

    random: std.rand.Random,
    cluster: *Cluster,
    workload: *StateMachine.Workload,
    options: Options,
    message_pool: MessagePool,

    /// The next op to be verified.
    /// Starts at 1, because op=0 is the root.
    stalled_op: u64 = 1,

    /// The list of messages waiting to be verified (the reply for a lower op has not yet arrived).
    /// Includes `register` messages.
    stalled_queue: PendingReplyQueue,

    /// Total number of messages sent, including those that have not been delivered.
    /// Does not include `register` messages.
    requests_sent: usize = 0,

    idle: bool = false,

    const Options = struct {
        /// The total number of requests to send. Does not count `register` messages.
        requests_max: usize,

        request_probability: u8, // percent
        idle_on_probability: u8, // percent
        idle_off_probability: u8, // percent
    };

    pub fn init(
        allocator: std.mem.Allocator,
        random: std.rand.Random,
        cluster: *Cluster,
        workload: *StateMachine.Workload,
        options: Options,
    ) !Conductor {
        assert(options.requests_max > 0);

        assert(options.request_probability > 0);
        assert(options.request_probability <= 100);
        assert(options.idle_on_probability <= 100);
        assert(options.idle_off_probability > 0);
        assert(options.idle_off_probability <= 100);

        assert(cluster.clients.len * 2 < stalled_queue_capacity);

        // *2 for PendingReply.request and PendingReply.reply.
        var message_pool = try MessagePool.init_capacity(allocator, stalled_queue_capacity * 2);
        errdefer message_pool.deinit(allocator);

        var stalled_queue = PendingReplyQueue.init(allocator, {});
        errdefer stalled_queue.deinit();
        try stalled_queue.ensureTotalCapacity(stalled_queue_capacity);

        return Conductor{
            .random = random,
            .cluster = cluster,
            .workload = workload,
            .options = options,
            //.client_id_permutation = client_id_permutation,
            //.clients = clients,
            //.client_pools = client_pools,
            .message_pool = message_pool,
            .stalled_queue = stalled_queue,
        };
    }

    pub fn deinit(conductor: *Conductor, allocator: std.mem.Allocator) void {
        while (conductor.stalled_queue.removeOrNull()) |pending| {
            conductor.message_pool.unref(pending.request);
            conductor.message_pool.unref(pending.reply);
        }
        conductor.stalled_queue.deinit();
        conductor.message_pool.deinit(allocator);
    }

    /// The conductor is "done" when it has delivered `requests_max` requests and received all
    /// replies.
    pub fn done(conductor: *const Conductor) bool {
        assert(conductor.requests_sent <= conductor.options.requests_max);
        if (conductor.requests_sent < conductor.options.requests_max) return false;

        for (conductor.cluster.clients) |*client| {
            if (client.request_queue.count > 0) return false;
        }

        assert(conductor.workload.done());
        return true;
    }

    pub fn tick(conductor: *Conductor) void {
        if (conductor.done()) return;

        // Try to pick a client & queue a request.

        if (conductor.idle) {
            if (chance(conductor.random, conductor.options.idle_off_probability)) conductor.idle = false;
        } else {
            if (chance(conductor.random, conductor.options.idle_on_probability)) conductor.idle = true;
        }
        if (conductor.idle) return;
        if (!chance(conductor.random, conductor.options.request_probability)) return;

        if (conductor.requests_sent == conductor.options.requests_max) return;
        assert(conductor.requests_sent < conductor.options.requests_max);

        // Messages aren't added to `stalled_queue` until a reply arrives.
        // Before sending a new message, make sure there will definitely be room for it.
        var reserved: usize = 0;
        for (conductor.cluster.clients) |*c| {
            // Count the number of clients that are still waiting for a `register` to complete,
            // since they may start one at any time.
            reserved += @boolToInt(c.session == 0);
            // Count the number of requests queued.
            reserved += c.request_queue.count;
        }

        // +1 for the potential request â€” is there room in our queue?
        if (conductor.stalled_queue.len + reserved + 1 > stalled_queue_capacity) return;

        const client_index = conductor.random.uintLessThanBiased(usize, conductor.cluster.clients.len);
        var client = &conductor.cluster.clients[client_index];

        // Check for space in the client's own request queue.
        if (client.request_queue.count + 1 > constants.client_request_queue_max) return;

        var request_message = client.get_message();
        defer client.unref(request_message);

        const request_metadata = conductor.workload.build_request(
            client_index,
            @alignCast(
                @alignOf(vsr.Header),
                request_message.buffer[@sizeOf(vsr.Header)..constants.message_size_max],
            ),
        );
        assert(request_metadata.size <= constants.message_size_max - @sizeOf(vsr.Header));

        conductor.cluster.request(
            client_index,
            request_metadata.operation,
            request_message,
            request_metadata.size,
        );
        // Since we already checked the client's request queue for free space, `client.request()`
        // should always queue the request.
        assert(request_message == client.request_queue.tail_ptr().?.message);
        assert(request_message.header.client == client.id);
        assert(request_message.header.request == client.request_number - 1);
        assert(request_message.header.size == @sizeOf(vsr.Header) + request_metadata.size);
        assert(request_message.header.operation.cast(StateMachine) == request_metadata.operation);

        conductor.requests_sent += 1;
        assert(conductor.requests_sent <= conductor.options.requests_max);
    }

    pub fn reply(
        conductor: *Conductor,
        client_index: usize,
        request_message: *Message,
        reply_message: *Message,
    ) void {
        const client = &conductor.cluster.clients[client_index];
        assert(reply_message.header.cluster == conductor.cluster.options.cluster_id);
        assert(reply_message.header.invalid() == null);
        assert(reply_message.header.client == client.id);
        assert(reply_message.header.request == request_message.header.request);
        assert(reply_message.header.op >= conductor.stalled_op);
        assert(reply_message.header.command == .reply);
        assert(reply_message.header.operation == request_message.header.operation);

        conductor.stalled_queue.add(.{
            .client_index = client_index,
            .request = conductor.clone_message(request_message),
            .reply = conductor.clone_message(reply_message),
        }) catch unreachable;

        if (reply_message.header.op == conductor.stalled_op) {
            conductor.consume_stalled_replies();
        }
    }

    /// Copy the message from a Client's MessagePool to the Conductor's MessagePool.
    ///
    /// The client has a finite amount of messages in its pool, and the Conductor needs to hold
    /// onto requests/replies until all preceeding requests/replies have arrived.
    ///
    /// Returns the Conductor's message.
    fn clone_message(conductor: *Conductor, message_client: *const Message) *Message {
        const message_conductor = conductor.message_pool.get_message();
        util.copy_disjoint(.exact, u8, message_conductor.buffer, message_client.buffer);
        return message_conductor;
    }

    fn consume_stalled_replies(conductor: *Conductor) void {
        assert(conductor.stalled_queue.len > 0);
        assert(conductor.stalled_queue.len <= stalled_queue_capacity);
        while (conductor.stalled_queue.peek()) |head| {
            assert(head.reply.header.op >= conductor.stalled_op);
            if (head.reply.header.op != conductor.stalled_op) break;

            const commit = conductor.stalled_queue.remove();
            defer conductor.message_pool.unref(commit.reply);
            defer conductor.message_pool.unref(commit.request);

            const client = conductor.cluster.clients[commit.client_index];
            assert(commit.reply.references == 1);
            assert(commit.reply.header.command == .reply);
            assert(commit.reply.header.client == client.id);
            assert(commit.reply.header.request == commit.request.header.request);
            assert(commit.reply.header.op == conductor.stalled_op);
            assert(commit.reply.header.operation == commit.request.header.operation);

            assert(commit.request.references == 1);
            assert(commit.request.header.command == .request);
            assert(commit.request.header.client == client.id);

            log.debug("consume_stalled_replies: op={} operation={} client={} request={}", .{
                commit.reply.header.op,
                commit.reply.header.operation,
                commit.request.header.client,
                commit.request.header.request,
            });

            if (commit.request.header.operation != .register) {
                conductor.workload.on_reply(
                    commit.client_index,
                    commit.reply.header.operation,
                    commit.reply.header.timestamp,
                    commit.request.body(),
                    commit.reply.body(),
                );
            }
            conductor.stalled_op += 1;
        }
    }
};

/// Returns true, `p` percent of the time, else false.
fn chance(random: std.rand.Random, p: u8) bool {
    assert(p <= 100);
    return random.uintLessThanBiased(u8, 100) < p;
}
